import pytz

# –ü–∞—Ç—á –¥–ª—è astimezone
def patched_astimezone(tz):
    if tz is None:
        from tzlocal import get_localzone
        tz = get_localzone()
    if hasattr(tz, 'zone'):
        return tz
    try:
        return pytz.timezone(str(tz))
    except Exception:
        raise TypeError('Only timezones from the pytz library are supported')

import apscheduler.util
apscheduler.util.astimezone = patched_astimezone

# –¢–µ–ø–µ—Ä —ñ–º–ø–æ—Ä—Ç—É—î–º–æ Telegram —ñ –≤—Å–µ —ñ–Ω—à–µ
import logging
import joblib
import pandas as pd
from scipy.sparse import hstack
from telegram import Update
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    MessageHandler,
    ContextTypes,
    filters,
)
import asyncio

#–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
model = joblib.load('fake_news_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')

#–°–ª–æ–≤–∞-–∫–ª—ñ–∫–±–µ–π—Ç–∏
clickbait_words = [
    "shocking", "scandal", "unbelievable", "you won‚Äôt believe", "amazing", "incredible",
    "top secret", "revealed", "finally exposed", "exposed", "the truth about", "what happened next",
    "goes viral", "breaks the internet", "jaw dropping", "insane", "crazy", "epic", "this will change everything",
    "life-changing", "must see", "watch now", "nobody saw this coming", "hidden truth", "banned", "illegal",
    "this one trick", "doctors hate", "before it's deleted", "this is why", "number one reason", "you need to know",
    "don‚Äôt ignore", "can‚Äôt believe", "warning", "alert", "urgent", "explosive", "shocking discovery",
    "outrage", "meltdown", "insider info", "hack", "leak", "conspiracy", "controversial", "exposed secret",
    "government cover-up", "game changer"
]

#–†—É—á–Ω—ñ –æ–∑–Ω–∞–∫–∏
def hand_crafted_features(text_series):
    df = pd.DataFrame({'text': text_series})
    df['caps'] = df['text'].apply(lambda x: int(x.isupper()) if isinstance(x, str) else 0)
    df['excl'] = df['text'].apply(lambda x: x.count('!') if isinstance(x, str) else 0)
    df['clickbait'] = df['text'].apply(
        lambda x: sum(word in x.lower() for word in clickbait_words) if isinstance(x, str) else 0
    )
    df['link'] = df['text'].apply(lambda x: int("http" in x or "www" in x) if isinstance(x, str) else 0)
    return df[['caps', 'excl', 'clickbait', 'link']]

#–ê–Ω–∞–ª—ñ–∑ —Ç–µ–∫—Å—Ç—É
def predict_fake_news(text):
    series = pd.Series([text])
    tfidf = vectorizer.transform(series)
    features = hand_crafted_features(series)
    full = hstack([tfidf, features.values])
    prediction = model.predict(full)[0]
    prob = model.predict_proba(full)[0][prediction]
    label = "üü© * –ü—Ä–∞–≤–¥–∏–≤–∞ –Ω–æ–≤–∏–Ω–∞ *" if prediction == 0 else "üü• * –§–µ–π–∫–æ–≤–∞ –Ω–æ–≤–∏–Ω–∞ *"
    return f"{label}\n\n–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å: `{prob:.2f}`"

#–ö–æ–º–∞–Ω–¥–∏
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("üëã –ù–∞–¥—ñ—à–ª–∏ –º–µ–Ω—ñ —Ç–µ–∫—Å—Ç –Ω–æ–≤–∏–Ω–∏, –∞ —è —Å–∫–∞–∂—É —á–∏ –≤–æ–Ω–∞ —Ñ–µ–π–∫–æ–≤–∞.")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    text = update.message.text
    result = predict_fake_news(text)
    await update.message.reply_markdown(result)
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üëã –ü—Ä–∏–≤—ñ—Ç! –ù–∞–¥—ñ—à–ª–∏ –º–µ–Ω—ñ —Ç–µ–∫—Å—Ç –Ω–æ–≤–∏–Ω–∏, –∞ —è —Å–∫–∞–∂—É —á–∏ –≤–æ–Ω–∞ —Ñ–µ–π–∫–æ–≤–∞."
    )

if __name__ == '__main__':
    TOKEN = "7691365421:AAHvtKf7ndH9y4KmE32HG7fRzVNbwWo7xE8"
    app = ApplicationBuilder().token(TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    print("‚úÖ –ë–æ—Ç SuperficialInspector –∑–∞–ø—É—â–µ–Ω–æ")
    asyncio.run(app.run_polling())
